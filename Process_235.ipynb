{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search for import above the current directory\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "cwd = Path().resolve().parent\n",
    "\n",
    "sys.path.insert(1, os.path.join(cwd, 'Read Process FT Stat'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dataAnalyzer_FTStat\n",
    "import dataScreen_FTStat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240731_93_DDT_SIG_EI_15k_1e6_500_1.txt\n",
      "20240731_94_DDT_SIG_EI_15k_1e6_500_2.txt\n",
      "20240731_95_DDT_SIG_EI_15k_1e6_500_3.txt\n",
      "20240731_96_DDT_SIG_EI_15k_1e6_500_4.txt\n",
      "20240731_97_DDT_SIG_EI_15k_1e6_500_5.txt\n",
      "20240731_98_DDT_SIG_EI_15k_1e6_500_6.txt\n",
      "20240731_99_DDT_SIG_EI_15k_1e6_500_7.txt\n",
      "20240801_113_DDT_SIG_EI_15k_1e6_500_1.txt\n",
      "20240801_114_DDT_LGC_EI_15k_1e6_500_2.txt\n",
      "20240801_115_DDT_SIG_EI_15k_1e6_500_3.txt\n",
      "20240801_116_DDT_LGC_EI_15k_1e6_500_4.txt\n",
      "20240801_117_DDT_SIG_EI_15k_1e6_500_5.txt\n",
      "20240801_118_DDT_LGC_EI_15k_1e6_500_6.txt\n",
      "20240801_119_DDT_SIG_EI_15k_1e6_500_7.txt\n",
      "20240802_144_DDT_SIG_EI_Reservoir_235_1.txt\n",
      "20240802_145_DDT_AGI_EI_Reservoir_235_2.txt\n",
      "20240802_146_DDT_SIG_EI_Reservoir_235_3.txt\n",
      "20240802_147_DDT_AGI_EI_Reservoir_235_4.txt\n",
      "20240802_148_DDT_SIG_EI_Reservoir_235_5.txt\n",
      "20240802_149_DDT_AGI_EI_Reservoir_235_6.txt\n",
      "20240802_150_DDT_SIG_EI_Reservoir_235_7.txt\n",
      "20241014_49_DDT_25uM_RES_235_1.txt\n",
      "20241014_50_DDT_40_RES_235_1.txt\n",
      "20241014_51_DDT_25uM_RES_235_2.txt\n",
      "20241014_55_DDT_25uM_RES_235_1_second_run_repeat.txt\n",
      "20241014_56_DDT_40_RES_235_1_second_run.txt\n",
      "20241014_57_DDT_25uM_RES_235_2_second_run.txt\n",
      "20241014_65_DDT_25uM_RES_235_1_third_run.txt\n",
      "20241014_66_DDT_40_RES_235_1_third_run.txt\n",
      "20241014_67_DDT_25uM_RES_235_2_third_run.txt\n",
      "File 1 235 37Cl-37Cl-13C/Unsub fails RSE/SN Test with value of 5.584127766539409\n",
      "File 4 235 37Cl-37Cl-13C/Unsub fails RSE/SN Test with value of 2.9626999751467555\n",
      "File 7 235 37Cl-37Cl-13C/Unsub fails RSE/SN Test with value of 2.8083603634609173\n"
     ]
    }
   ],
   "source": [
    "#Folder with FT Statistic-ified files. All the files need to be processed using the same metrics.\n",
    "fragKey = '235'\n",
    "toProcess = {'20240731_235_ZeroEnrichment':{'smp Indices':[1,3,5]},\n",
    "             '20240801_235_LGC':{'smp Indices':[1,3,5]},\n",
    "             '20240802_235_AGI':{'smp Indices':[1,3,5]},\n",
    "             '20241027_235_40':{'smp Indices':[1,4,7]}}\n",
    "\n",
    "outDict = {}\n",
    "allMergedDict = {}\n",
    "for folderPath, thisFolderParams in toProcess.items():\n",
    "    fragmentDict = {'235':['Unsub','13C','37Cl','37Cl-13C','37Cl-37Cl','37Cl-37Cl-13C']}\n",
    "    fragmentMostAbundant = ['Unsub']\n",
    "\n",
    "    massStr = []\n",
    "    fragmentIsotopeList = []\n",
    "    for i, v in fragmentDict.items():\n",
    "        massStr.append(i)\n",
    "        fragmentIsotopeList.append(v)\n",
    "        \n",
    "    cullByTime = True\n",
    "    cullTimes = [(19.0,40.0)]\n",
    "    #If cullTime left bound is <19.0, can mess with the program to determine cull times by identifying an abundant 'Unsub' peak eluting before the valves have been switched, e.g., due to another compound which makes the same fragment eluting earlier in direct elution mode\n",
    "\n",
    "    rtnAllFilesDF, mergedDict, allOutputDict = dataAnalyzer_FTStat.calc_Folder_Output(folderPath, cullOn='TIC*IT',cullAmount = 3, \n",
    "                                                cullByTime=True, cullTimes = cullTimes, \n",
    "                                                fragmentIsotopeList = fragmentIsotopeList, \n",
    "                                                fragmentMostAbundant = fragmentMostAbundant, debug = True, fileExt = '.txt', \n",
    "                                               massStrList = list(fragmentDict.keys()),\n",
    "                                              Microscans = 1,\n",
    "                                             minNL_over_maxNL = 0.2,\n",
    "                                             cull_on_IT = 3000)\n",
    "    \n",
    "    dataScreen_FTStat.zeroCountsScreen(mergedDict, fragmentDict[fragKey])\n",
    "    dataScreen_FTStat.RSESNScreen(allOutputDict)\n",
    "    \n",
    "    outDict[folderPath] = rtnAllFilesDF\n",
    "    allMergedDict[folderPath] = copy.deepcopy(mergedDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240731_93_DDT_SIG_EI_15k_1e6_500_1.txt\n",
      "(19.35, 25.75)\n",
      "20240731_94_DDT_SIG_EI_15k_1e6_500_2.txt\n",
      "(19.35, 25.73)\n",
      "20240731_95_DDT_SIG_EI_15k_1e6_500_3.txt\n",
      "(19.34, 25.8)\n",
      "20240731_96_DDT_SIG_EI_15k_1e6_500_4.txt\n",
      "(19.35, 25.33)\n",
      "20240731_97_DDT_SIG_EI_15k_1e6_500_5.txt\n",
      "(19.36, 24.7)\n",
      "20240731_98_DDT_SIG_EI_15k_1e6_500_6.txt\n",
      "(19.34, 25.85)\n",
      "20240731_99_DDT_SIG_EI_15k_1e6_500_7.txt\n",
      "(19.35, 25.86)\n",
      "20240801_113_DDT_SIG_EI_15k_1e6_500_1.txt\n",
      "(19.33, 25.85)\n",
      "20240801_114_DDT_LGC_EI_15k_1e6_500_2.txt\n",
      "(19.33, 25.87)\n",
      "20240801_115_DDT_SIG_EI_15k_1e6_500_3.txt\n",
      "(19.33, 25.93)\n",
      "20240801_116_DDT_LGC_EI_15k_1e6_500_4.txt\n",
      "(19.33, 25.9)\n",
      "20240801_117_DDT_SIG_EI_15k_1e6_500_5.txt\n",
      "(19.33, 25.9)\n",
      "20240801_118_DDT_LGC_EI_15k_1e6_500_6.txt\n",
      "(19.33, 25.9)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20240801_119_DDT_SIG_EI_15k_1e6_500_7.txt\n",
      "(19.33, 25.94)\n",
      "20240802_144_DDT_SIG_EI_Reservoir_235_1.txt\n",
      "(19.34, 25.98)\n",
      "20240802_145_DDT_AGI_EI_Reservoir_235_2.txt\n",
      "(19.35, 26.03)\n",
      "20240802_146_DDT_SIG_EI_Reservoir_235_3.txt\n",
      "(19.34, 25.97)\n",
      "20240802_147_DDT_AGI_EI_Reservoir_235_4.txt\n",
      "(19.35, 25.98)\n",
      "20240802_148_DDT_SIG_EI_Reservoir_235_5.txt\n",
      "(19.34, 26.01)\n",
      "20240802_149_DDT_AGI_EI_Reservoir_235_6.txt\n",
      "(19.35, 25.99)\n",
      "20240802_150_DDT_SIG_EI_Reservoir_235_7.txt\n",
      "(19.34, 26.09)\n",
      "20241014_49_DDT_25uM_RES_235_1.txt\n",
      "(19.23, 24.41)\n",
      "20241014_50_DDT_40_RES_235_1.txt\n",
      "(19.25, 24.49)\n",
      "20241014_51_DDT_25uM_RES_235_2.txt\n",
      "(19.26, 24.4)\n",
      "20241014_55_DDT_25uM_RES_235_1_second_run_repeat.txt\n",
      "(19.28, 24.31)\n",
      "20241014_56_DDT_40_RES_235_1_second_run.txt\n",
      "(19.23, 24.34)\n",
      "20241014_57_DDT_25uM_RES_235_2_second_run.txt\n",
      "(19.27, 24.21)\n",
      "20241014_65_DDT_25uM_RES_235_1_third_run.txt\n",
      "(19.3, 24.26)\n",
      "20241014_66_DDT_40_RES_235_1_third_run.txt\n",
      "(19.22, 24.37)\n",
      "20241014_67_DDT_25uM_RES_235_2_third_run.txt\n",
      "(19.27, 24.25)\n"
     ]
    }
   ],
   "source": [
    "#can check manually for oddities\n",
    "for runKey, runData in allMergedDict.items():\n",
    "\tfor fileKey, fileData in runData.items():\n",
    "\t\tprint(fileKey)\n",
    "\t\tprint(str((fileData[0].retTime.min(), fileData[0].retTime.max())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate standardized deltas and propagated errors.\n",
    "allFolderValues = {}\n",
    "for folderPath, folderParams in toProcess.items():\n",
    "\tthisRtnAllFilesDf = outDict[folderPath]\n",
    "\tsampleOutputDict = dataAnalyzer_FTStat.folderOutputToDict(thisRtnAllFilesDf)\n",
    "\n",
    "\tfile_keys = list(sampleOutputDict.keys())\n",
    "\tsub_keys = list(sampleOutputDict[file_keys[0]][fragKey].keys())\n",
    "\t  \n",
    "\tthisFolderValues = {}\n",
    "\tfor subIdx, subKey in enumerate(sub_keys):\n",
    "\t\t# Select the data for one subKey ('13C/Unsub') across all files\n",
    "\t\taverages = [sampleOutputDict[file][fragKey][subKey]['Average'] for file in sampleOutputDict]\n",
    "\t\trel_std_errors = [sampleOutputDict[file][fragKey][subKey]['RelStdError'] for file in sampleOutputDict]\n",
    "\t\t\t\t\n",
    "\t\t# Indices for samples\n",
    "\t\tsample_indices = folderParams['smp Indices']\n",
    "\n",
    "\t\t# Initialize lists for standardized values and propagated errors\n",
    "\t\tstandardized_deltas = []\n",
    "\t\tpropagated_errors = []\n",
    "\n",
    "\t\t# Function to calculate the propagated error\n",
    "\t\tdef calculate_propagated_error(sample_error, std1_error, std2_error):\n",
    "\t\t\treturn np.sqrt(sample_error**2 + 1/ np.sqrt(2) * (std1_error/2 + std2_error/2)**2)\n",
    "\n",
    "\t\t# Iterate through sample indices to calculate standardized values and errors\n",
    "\t\tfor idx in sample_indices:\n",
    "\t\t\tsample_value = averages[idx]\n",
    "\t\t\tstd1_value, std2_value = averages[idx - 1], averages[idx + 1]\n",
    "\t\t\t\n",
    "\t\t\tstandardized_value = (sample_value / std1_value + sample_value / std2_value) / 2\n",
    "\t\t\tstandardized_delta = 1000 * (standardized_value-1)\n",
    "\t\t\t\n",
    "\t\t\tsample_error = rel_std_errors[idx]\n",
    "\t\t\tstd1_error, std2_error = rel_std_errors[idx - 1], rel_std_errors[idx + 1]\n",
    "\t\t\t\n",
    "\t\t\tpropagated_error = calculate_propagated_error(sample_error, std1_error, std2_error)\n",
    "\t\t\tpropagated_error*= 1000\n",
    "\t\t\t\n",
    "\t\t\tstandardized_deltas.append(standardized_delta)\n",
    "\t\t\tpropagated_errors.append(propagated_error)\n",
    "\t\t\t\n",
    "\t\n",
    "\t\tthisFolderValues[subKey] = {'Deltas': copy.deepcopy(standardized_deltas), \n",
    "\t\t\t\t\t\t\t  'Propagated Acquisition Errors':copy.deepcopy(propagated_errors),\n",
    "\t\t\t\t\t\t\t  'Mean Delta' : np.array(standardized_deltas).mean(),\n",
    "\t\t\t\t\t\t\t  'Experimental Reproducibility': np.array(standardized_deltas).std() / np.sqrt(len(standardized_deltas))}\n",
    "\t\t\n",
    "\tallFolderValues[folderPath] = thisFolderValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUND\n"
     ]
    }
   ],
   "source": [
    "#Calculate experimental reproducibilities and their means\n",
    "# --- Step 1: Collect all reproducibilities grouped by Ratio Key ---\n",
    "from collections import defaultdict\n",
    "\n",
    "grouped_repro = defaultdict(list)\n",
    "\n",
    "for folder, ratios in allFolderValues.items():\n",
    "    if folder == '20241027_235_40':\n",
    "        print(\"FOUND\")\n",
    "        continue\n",
    "    for ratio_key, metrics in ratios.items():\n",
    "        grouped_repro[ratio_key].append(metrics['Experimental Reproducibility'])\n",
    "\n",
    "# --- Step 2: Compute mean for each Ratio Key ---\n",
    "mean_repro_by_key = {k: sum(v) / len(v) for k, v in grouped_repro.items()}\n",
    "\n",
    "# --- Step 3: Add it back to each entry ---\n",
    "for folder, ratios in allFolderValues.items():\n",
    "    for ratio_key, metrics in ratios.items():\n",
    "        if folder == '20241027_235_40':\n",
    "            this_MR = metrics['Experimental Reproducibility']\n",
    "        else:\n",
    "            this_MR = mean_repro_by_key[ratio_key]\n",
    "        metrics['Mean Experimental Reproducibility'] = this_MR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save to JSON file\n",
    "with open('235_data.json', 'w') as f:\n",
    "    json.dump(allFolderValues, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to CSV\n",
    "# Step 1: Flatten the data\n",
    "flattened_data = []\n",
    "for folder_name, ratios in allFolderValues.items():\n",
    "    for ratio_key, metrics in ratios.items():\n",
    "        entry = {\n",
    "            'Folder Name': folder_name,\n",
    "            'Ratio Key': ratio_key,\n",
    "            'Deltas': metrics['Deltas'],\n",
    "            'Propagated Acquisition Errors': metrics['Propagated Acquisition Errors'],\n",
    "            'Mean Delta': metrics['Mean Delta'],\n",
    "            'Experimental Reproducibility': metrics['Experimental Reproducibility'],\n",
    "            'Mean Experimental Reproducibility': metrics['Mean Experimental Reproducibility']\n",
    "        }\n",
    "        flattened_data.append(entry)\n",
    "\n",
    "# Step 2: Create a pandas DataFrame\n",
    "df = pd.DataFrame(flattened_data)\n",
    "\n",
    "# Step 3: Expand lists into separate columns\n",
    "df = df.explode(['Deltas', 'Propagated Acquisition Errors']).reset_index(drop=True)\n",
    "\n",
    "df.to_csv(\"235 Data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.901506\n",
       "1      0.864005\n",
       "2      0.888115\n",
       "3      1.585616\n",
       "4      1.555064\n",
       "5      1.590096\n",
       "6      0.837037\n",
       "7      0.816104\n",
       "8      0.824998\n",
       "9      2.074867\n",
       "10     2.008073\n",
       "11     2.028436\n",
       "12     0.595648\n",
       "13     0.578228\n",
       "14     0.583244\n",
       "15     1.442714\n",
       "16     1.445204\n",
       "17     1.447202\n",
       "18     1.110816\n",
       "19     1.124072\n",
       "20     1.128735\n",
       "21      0.56046\n",
       "22     0.559597\n",
       "23     0.556995\n",
       "24     0.539105\n",
       "25     0.539605\n",
       "26     0.540255\n",
       "27     0.523971\n",
       "28     0.526324\n",
       "29     0.528024\n",
       "30     0.244928\n",
       "31     0.242183\n",
       "32     0.242922\n",
       "33     0.961356\n",
       "34     0.957418\n",
       "35      0.95242\n",
       "36     0.360686\n",
       "37     0.357644\n",
       "38     0.357284\n",
       "39     1.176678\n",
       "40     1.182788\n",
       "41     1.176803\n",
       "42     1.157252\n",
       "43     1.150603\n",
       "44     1.150804\n",
       "45     0.569819\n",
       "46     0.484739\n",
       "47     0.512723\n",
       "48     1.304471\n",
       "49     1.089758\n",
       "50     1.212337\n",
       "51    27.488654\n",
       "52    40.276443\n",
       "53     46.84369\n",
       "54     0.912431\n",
       "55     0.732125\n",
       "56      0.80379\n",
       "57     1.199631\n",
       "58     0.984035\n",
       "59     1.037329\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check ER vs propagated acquisition errors\n",
    "# \n",
    "df['Experimental Reproducibility'] * np.sqrt(3) / df['Propagated Acquisition Errors']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Orbitrap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
